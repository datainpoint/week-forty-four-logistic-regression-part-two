{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2503f0bd",
   "metadata": {},
   "source": [
    "# 約維安計畫：羅吉斯迴歸（下）\n",
    "\n",
    "> 第四十四週\n",
    "\n",
    "![giphy.com](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExcnpqbXE0YjhxbHFpMmM0OXA3YWxqcjR1ZTJmaTF4dTk2cWZmbWQ4NyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/GSbq5jLhJJWrewuzRG/giphy.gif)\n",
    "\n",
    "來源：<https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExcnpqbXE0YjhxbHFpMmM0OXA3YWxqcjR1ZTJmaTF4dTk2cWZmbWQ4NyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/GSbq5jLhJJWrewuzRG/giphy.gif>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fb4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596539cf",
   "metadata": {},
   "source": [
    "## 羅吉斯迴歸的成本函數\n",
    "\n",
    "在[約維安計畫：羅吉斯迴歸（上）](https://datainpoint.substack.com/p/week-forty-three-logistic-regression-part-one)一文中我們提到了羅吉斯迴歸類別預測模型的定義、交叉熵（cross entropy）成本函數、Sigmoid 函數以及如何運用梯度遞減（gradient descent）學習演算方法求解權重向量。\n",
    "\n",
    "\\begin{gather}\n",
    "\\text{For each epoch:} \\\\\n",
    "w := w - \\eta \\nabla_w J(w) \\; \\text{, where } \\eta \\in \\mathbb{R}^+ \n",
    "\\end{gather}\n",
    "\n",
    "在這個式子中最重要的一個求解任務就是成本函數 J 關於權重向量的梯度，那麼這個成本函數 J，也就是交叉熵成本函數的偏微該怎麼推導呢？首先我們將成本函數 J 的內容再細寫一次：\n",
    "\n",
    "\\begin{gather}\n",
    "z = Xw \\text{,}\\; X \\in \\mathbb{R^{m \\times n}} \\text{,} \\; w \\in \\mathbb{R^n}\\\\\n",
    "\\hat{p} = \\sigma(z) \\text{, where } \\sigma(x) = \\frac{1}{1 + e^{-x}} \\\\\n",
    "J(\\hat{p}) = \\frac{1}{m} \\left( -ylog(\\hat{p}) - (1-y)log(1-\\hat{p}) \\right) \\\\\n",
    "J(w) = \\frac{1}{m} \\left( -ylog(\\sigma(Xw)) - (1-y)log(1-\\sigma(Xw)) \\right)\n",
    "\\end{gather}\n",
    "\n",
    "仔細檢查成本函數 J 的組成，我們會發現這是一個鏈結函數（chaining function）的結構，內層有對特徵矩陣和權重向量相乘結果應用 Sigmoid 函數，外層則有對 Sigmoid 函數輸出應用對數函數，若是想順利推導其成本函數的梯度，我們可以預期必須具有幾項先備知識：連鎖法則（chain rule）、對數函數微分以及 Sigmoid 函數微分。\n",
    "\n",
    "## 連鎖法則\n",
    "\n",
    "連鎖法則可以用於求解鏈結函數的導數，如果一個鏈結函數 h 是由 g 和 f 兩函數依序作用合成，g 函數先作用，其輸出再成為 f 函數的輸入，那麼 h 函數關於 x 的導數為兩個微分結果的相乘，依序是 h 函數關於 g 函數的微分、g 函數關於 x 的微分。\n",
    "\n",
    "\\begin{gather}\n",
    "h(x) = f(g(x)) \\\\\n",
    "h = f \\circ g \\\\\n",
    "h'(x) = f'(g(x))g'(x) \\\\\n",
    "\\frac{dh}{dx} = \\frac{d h}{dg}\\frac{dg}{dx}\n",
    "\\end{gather}\n",
    "\n",
    "將連鎖法則應用於成本函數 J 的梯度求解，可以將其寫成兩個偏微分結果的結合，依序是成本函數 J 關於對數函數的偏微分、對數函數關於權重向量的偏微分。接下來我們可以拆開來解這兩個偏微分的部分，最後結合起來得到想要求解的「成本函數 J 關於權重向量的梯度」。\n",
    "\n",
    "## 對數函數偏微分\n",
    "\n",
    "關於對數函數偏微分，先備知識（不另外證明的部分）為：\n",
    "\n",
    "\\begin{gather}\n",
    "\\frac{d}{dx}log(x) = \\frac{1}{x} \\\\\n",
    "\\frac{d}{dx}log(1-x) = \\frac{-1}{1-x}\n",
    "\\end{gather}\n",
    "\n",
    "因此我們可以先將成本函數 J 關於權重向量的偏微分推導為成本函數 J 關於對數函數的偏微分形式：\n",
    "\n",
    "\\begin{gather}\n",
    "J(w) = \\frac{1}{m} \\left( -ylog(\\hat{p}) - (1-y)log(1-\\hat{p}) \\right)\\\\\n",
    "\\nabla_w J(w) = \\frac{1}{m} \\left( -y\\frac{1}{\\hat{p}} \\frac{\\partial \\hat{p}}{\\partial z} - (1-y)\\frac{-1}{1-\\hat{p}} \\frac{\\partial(1-\\hat{p})}{\\partial z} \\right)\n",
    "\\end{gather}\n",
    "\n",
    "完成對數函數偏微分的部分，接下來是 Sigmoid 函數偏微分的部分。\n",
    "\n",
    "## Sigmoid 函數偏微分\n",
    "\n",
    "關於 Sigmoid 函數偏微分，先備知識（不另外證明的部分）為：\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{dx}\\sigma(x) &= \\frac{d}{dx} \\left( (1+e^{-x})^{-1} \\right) \\\\\n",
    "&= \\frac{0 \\cdot (1+e^{-x})-1 \\cdot (e^{-x} \\cdot (-1) )}{(1 + e^{-x})^2}\\\\\n",
    "&= \\frac{e^{-x}}{(1+e^{-x})^2}\\\\\n",
    "&= \\frac{1-1+e^{-x}}{(1+e^{-x})^2}\\\\\n",
    "&= \\frac{1+e^{-x}}{(1+e^{-x})^2} - \\frac{1}{(1+e^{-x})^2} \\\\\n",
    "&= \\frac{1}{1+e^{-x}} - \\frac{1}{(1+e^{-x})^2} \\\\\n",
    "&= \\frac{1}{1+e^{-x}} \\left( 1 - \\frac{1}{1+e^{-x}} \\right) \\\\\n",
    "&= \\sigma(x)\\left( 1-\\sigma(x) \\right)\n",
    "\\end{align}\n",
    "\n",
    "先備知識中的第二式運用到微分的除法規則、第四式則運用到了一個巧妙的 +1-1 的技巧。\n",
    "\n",
    "\\begin{gather}\n",
    "h(x) = \\frac{f(x)}{g(x)}\\\\\n",
    "h'(x) = \\frac{f(x)g'(x) - f'(x)g(x)}{g^2(x)}\n",
    "\\end{gather}\n",
    "\n",
    "因此我們可以接著將成本函數 J 關於權重向量的偏微分推導為最終的形式：\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_w J(w) &= \\frac{1}{m} \\left( -y\\frac{1}{\\hat{p}} \\frac{\\partial \\hat{p}}{\\partial z} - (1-y)\\frac{-1}{1-\\hat{p}} \\frac{\\partial(1-\\hat{p})}{\\partial z} \\right) \\\\\n",
    "&= \\frac{1}{m} \\left( -y\\frac{1}{\\hat{p}} \\sigma(z)(1-\\sigma(z))\\sigma'(z) + (1-y)\\frac{1}{1-\\hat{p}}\\sigma(z)(1-\\sigma(z))\\sigma'(z)\\right) \\\\\n",
    "&= \\frac{1}{m}\\sigma'(z) \\left( -y\\frac{1}{\\sigma(z)} \\sigma(z)(1-\\sigma(z)) + (1-y)\\frac{1}{1-\\sigma(z)}\\sigma(z)(1-\\sigma(z))\\right) \\\\\n",
    "&= \\frac{1}{m}\\sigma'(z) \\left( -y(1-\\sigma(z)) + (1-y)\\sigma(z) \\right) \\\\\n",
    "&=\\frac{1}{m}\\sigma'(z)\\left( -y+y\\sigma(z) + \\sigma(z)-y\\sigma(z) \\right) \\\\\n",
    "&=\\frac{1}{m}\\sigma'(z)\\left(\\sigma(z)-y \\right)\\\\\n",
    "&=\\frac{1}{m}X^{\\top}\\left(\\sigma(Xw)-y \\right)\n",
    "\\end{align}\n",
    "\n",
    "我們終於推導出以交叉熵作為成本函數 J 關於權重向量的梯度，最後將羅吉斯迴歸梯度遞減的泛型式表示如下。\n",
    "\n",
    "\\begin{gather}\n",
    "\\text{For each epoch:} \\\\\n",
    "w := w - \\eta \\nabla_w J(w) \\; \\text{, where } \\eta \\in \\mathbb{R}^+ \\\\\n",
    "w := w - \\eta \\frac{1}{m}X^{\\top} \\left( \\sigma(Xw) - y \\right)\n",
    "\\end{gather}\n",
    "\n",
    "## 以 Numpy 實作羅吉斯迴歸\n",
    "\n",
    "成功推導出以交叉熵作為成本函數 J 關於權重向量的梯度之後，最後我們寫作一個類別 `LogisticRegression`，以 Python 與 Numpy 模組實作，在給定適當資料集的特徵矩陣與目標向量，能夠迭代優化隨機生成的權重向量，進而生成 h 函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd0a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    This class defines the vanilla descent algorithm for logistic regression.\n",
    "    Args:\n",
    "        fit_intercept (bool): Whether to add intercept for this model.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self._fit_intercept = fit_intercept\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        This function returns the Sigmoid output as a probability given certain model weights.\n",
    "        \"\"\"\n",
    "        X_w = np.dot(X, self._w)\n",
    "        p_hat = 1 / (1 + np.exp(-X_w))\n",
    "        return p_hat\n",
    "    def find_gradient(self):\n",
    "        \"\"\"\n",
    "        This function returns the gradient given certain model weights.\n",
    "        \"\"\"\n",
    "        m = self._m\n",
    "        p_hat = self.sigmoid(self._X_train)\n",
    "        X_train_T = np.transpose(self._X_train)\n",
    "        gradient = (1/m) * np.dot(X_train_T, p_hat - self._y_train)\n",
    "        return gradient\n",
    "    def cross_entropy(self, epsilon=1e-06):\n",
    "        \"\"\"\n",
    "        This function returns the cross entropy given certain model weights.\n",
    "        \"\"\"\n",
    "        m = self._m\n",
    "        p_hat = self.sigmoid(self._X_train)\n",
    "        cost_y1 = -np.dot(self._y_train, np.log(p_hat + epsilon))\n",
    "        cost_y0 = -np.dot(1 - self._y_train, np.log(1 - p_hat + epsilon))\n",
    "        cross_entropy = (cost_y1 + cost_y0) / m\n",
    "        return cross_entropy\n",
    "    def fit(self, X_train, y_train, epochs=10000, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        This function uses vanilla gradient descent to solve for weights of this model.\n",
    "        Args:\n",
    "            X_train (ndarray): 2d-array for feature matrix of training data.\n",
    "            y_train (ndarray): 1d-array for target vector of training data.\n",
    "            epochs (int): The number of iterations to update the model weights.\n",
    "            learning_rate (float): The learning rate of gradient descent.\n",
    "        \"\"\"\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        m = self._X_train.shape[0]\n",
    "        self._m = m\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((self._m, 1), dtype=float)\n",
    "            self._X_train = np.concatenate([X0, self._X_train], axis=1)\n",
    "        n = self._X_train.shape[1]\n",
    "        self._w = np.random.rand(n)\n",
    "        n_prints = 10\n",
    "        print_iter = epochs // n_prints\n",
    "        for i in range(epochs):\n",
    "            cross_entropy = self.cross_entropy()\n",
    "            gradient = self.find_gradient()\n",
    "            if i % print_iter == 0:\n",
    "                print(\"epoch: {:6} - loss: {:.6f}\".format(i, cross_entropy))\n",
    "            self._w -= learning_rate*gradient\n",
    "        w_ravel = self._w.ravel().copy()\n",
    "        self.intercept_ = w_ravel[0]\n",
    "        self.coef_ = w_ravel[1:].reshape(1, -1)\n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted probability with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        m = X_test.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_test = np.concatenate([X0, X_test], axis=1)\n",
    "        p_hat_1 = self.sigmoid(self._X_test).reshape(-1, 1)\n",
    "        p_hat_0 = 1 - p_hat_1\n",
    "        proba = np.concatenate([p_hat_0, p_hat_1], axis=1)\n",
    "        return proba\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted label with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X_test)\n",
    "        y_pred = np.argmax(proba, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2af0f",
   "metadata": {},
   "source": [
    "`LogisticRegression` 類別的使用是以 `fit()` 方法生成權重向量，並以 `predict()` 方法生成預測。我們可以將 `LogisticRegression` 類別應用在 Kaggle 的 Titanic 資料集，藉此來驗證它的可用性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4df32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:      0 - loss: 1.664711\n",
      "epoch:   1000 - loss: 0.814446\n",
      "epoch:   2000 - loss: 0.761798\n",
      "epoch:   3000 - loss: 0.724259\n",
      "epoch:   4000 - loss: 0.697649\n",
      "epoch:   5000 - loss: 0.678728\n",
      "epoch:   6000 - loss: 0.665173\n",
      "epoch:   7000 - loss: 0.655375\n",
      "epoch:   8000 - loss: 0.648231\n",
      "epoch:   9000 - loss: 0.642980\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/datainpoint/week-forty-four-logistic-regression-part-two/main/train.csv\")\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/datainpoint/week-forty-four-logistic-regression-part-two/main/test.csv\")\n",
    "X_train = train[\"Fare\"].values.reshape(-1, 1)\n",
    "y_train = train[\"Survived\"].values\n",
    "X_test = test[\"Fare\"].values.reshape(-1, 1)\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "y_pred = logistic_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68ba55",
   "metadata": {},
   "source": [
    "第四十四週約維安計畫：羅吉斯迴歸來到尾聲，希望您也和我一樣期待下一篇文章。對於這篇文章有什麼想法呢？喜歡😻、分享🙌、訂閱📨或者留言🙋‍♂️"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
