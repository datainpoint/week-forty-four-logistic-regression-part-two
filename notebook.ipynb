{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2503f0bd",
   "metadata": {},
   "source": [
    "# ç´„ç¶­å®‰è¨ˆç•«ï¼šç¾…å‰æ–¯è¿´æ­¸ï¼ˆä¸‹ï¼‰\n",
    "\n",
    "> ç¬¬å››åå››é€±\n",
    "\n",
    "![giphy.com](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExcnpqbXE0YjhxbHFpMmM0OXA3YWxqcjR1ZTJmaTF4dTk2cWZmbWQ4NyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/GSbq5jLhJJWrewuzRG/giphy.gif)\n",
    "\n",
    "ä¾†æºï¼š<https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExcnpqbXE0YjhxbHFpMmM0OXA3YWxqcjR1ZTJmaTF4dTk2cWZmbWQ4NyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/GSbq5jLhJJWrewuzRG/giphy.gif>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65fb4cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596539cf",
   "metadata": {},
   "source": [
    "## ç¾…å‰æ–¯è¿´æ­¸çš„æˆæœ¬å‡½æ•¸\n",
    "\n",
    "åœ¨[ç´„ç¶­å®‰è¨ˆç•«ï¼šç¾…å‰æ–¯è¿´æ­¸ï¼ˆä¸Šï¼‰](https://datainpoint.substack.com/p/week-forty-three-logistic-regression-part-one)ä¸€æ–‡ä¸­æˆ‘å€‘æåˆ°äº†ç¾…å‰æ–¯è¿´æ­¸é¡åˆ¥é æ¸¬æ¨¡å‹çš„å®šç¾©ã€äº¤å‰ç†µï¼ˆcross entropyï¼‰æˆæœ¬å‡½æ•¸ã€Sigmoid å‡½æ•¸ä»¥åŠå¦‚ä½•é‹ç”¨æ¢¯åº¦éæ¸›ï¼ˆgradient descentï¼‰å­¸ç¿’æ¼”ç®—æ–¹æ³•æ±‚è§£æ¬Šé‡å‘é‡ã€‚\n",
    "\n",
    "\\begin{gather}\n",
    "\\text{For each epoch:} \\\\\n",
    "w := w - \\eta \\nabla_w J(w) \\; \\text{, where } \\eta \\in \\mathbb{R}^+ \n",
    "\\end{gather}\n",
    "\n",
    "åœ¨é€™å€‹å¼å­ä¸­æœ€é‡è¦çš„ä¸€å€‹æ±‚è§£ä»»å‹™å°±æ˜¯æˆæœ¬å‡½æ•¸ J é—œæ–¼æ¬Šé‡å‘é‡çš„æ¢¯åº¦ï¼Œé‚£éº¼é€™å€‹æˆæœ¬å‡½æ•¸ Jï¼Œä¹Ÿå°±æ˜¯äº¤å‰ç†µæˆæœ¬å‡½æ•¸çš„åå¾®è©²æ€éº¼æ¨å°å‘¢ï¼Ÿé¦–å…ˆæˆ‘å€‘å°‡æˆæœ¬å‡½æ•¸ J çš„å…§å®¹å†ç´°å¯«ä¸€æ¬¡ï¼š\n",
    "\n",
    "\\begin{gather}\n",
    "z = Xw \\text{,}\\; X \\in \\mathbb{R^{m \\times n}} \\text{,} \\; w \\in \\mathbb{R^n}\\\\\n",
    "\\hat{p} = \\sigma(z) \\text{, where } \\sigma(x) = \\frac{1}{1 + e^{-x}} \\\\\n",
    "J(\\hat{p}) = \\frac{1}{m} \\left( -ylog(\\hat{p}) - (1-y)log(1-\\hat{p}) \\right) \\\\\n",
    "J(w) = \\frac{1}{m} \\left( -ylog(\\sigma(Xw)) - (1-y)log(1-\\sigma(Xw)) \\right)\n",
    "\\end{gather}\n",
    "\n",
    "ä»”ç´°æª¢æŸ¥æˆæœ¬å‡½æ•¸ J çš„çµ„æˆï¼Œæˆ‘å€‘æœƒç™¼ç¾é€™æ˜¯ä¸€å€‹éˆçµå‡½æ•¸ï¼ˆchaining functionï¼‰çš„çµæ§‹ï¼Œå…§å±¤æœ‰å°ç‰¹å¾µçŸ©é™£å’Œæ¬Šé‡å‘é‡ç›¸ä¹˜çµæœæ‡‰ç”¨ Sigmoid å‡½æ•¸ï¼Œå¤–å±¤å‰‡æœ‰å° Sigmoid å‡½æ•¸è¼¸å‡ºæ‡‰ç”¨å°æ•¸å‡½æ•¸ï¼Œè‹¥æ˜¯æƒ³é †åˆ©æ¨å°å…¶æˆæœ¬å‡½æ•¸çš„æ¢¯åº¦ï¼Œæˆ‘å€‘å¯ä»¥é æœŸå¿…é ˆå…·æœ‰å¹¾é …å…ˆå‚™çŸ¥è­˜ï¼šé€£é–æ³•å‰‡ï¼ˆchain ruleï¼‰ã€å°æ•¸å‡½æ•¸å¾®åˆ†ä»¥åŠ Sigmoid å‡½æ•¸å¾®åˆ†ã€‚\n",
    "\n",
    "## é€£é–æ³•å‰‡\n",
    "\n",
    "é€£é–æ³•å‰‡å¯ä»¥ç”¨æ–¼æ±‚è§£éˆçµå‡½æ•¸çš„å°æ•¸ï¼Œå¦‚æœä¸€å€‹éˆçµå‡½æ•¸ h æ˜¯ç”± g å’Œ f å…©å‡½æ•¸ä¾åºä½œç”¨åˆæˆï¼Œg å‡½æ•¸å…ˆä½œç”¨ï¼Œå…¶è¼¸å‡ºå†æˆç‚º f å‡½æ•¸çš„è¼¸å…¥ï¼Œé‚£éº¼ h å‡½æ•¸é—œæ–¼ x çš„å°æ•¸ç‚ºå…©å€‹å¾®åˆ†çµæœçš„ç›¸ä¹˜ï¼Œä¾åºæ˜¯ h å‡½æ•¸é—œæ–¼ g å‡½æ•¸çš„å¾®åˆ†ã€g å‡½æ•¸é—œæ–¼ x çš„å¾®åˆ†ã€‚\n",
    "\n",
    "\\begin{gather}\n",
    "h(x) = f(g(x)) \\\\\n",
    "h = f \\circ g \\\\\n",
    "h'(x) = f'(g(x))g'(x) \\\\\n",
    "\\frac{dh}{dx} = \\frac{d h}{dg}\\frac{dg}{dx}\n",
    "\\end{gather}\n",
    "\n",
    "å°‡é€£é–æ³•å‰‡æ‡‰ç”¨æ–¼æˆæœ¬å‡½æ•¸ J çš„æ¢¯åº¦æ±‚è§£ï¼Œå¯ä»¥å°‡å…¶å¯«æˆå…©å€‹åå¾®åˆ†çµæœçš„çµåˆï¼Œä¾åºæ˜¯æˆæœ¬å‡½æ•¸ J é—œæ–¼å°æ•¸å‡½æ•¸çš„åå¾®åˆ†ã€å°æ•¸å‡½æ•¸é—œæ–¼æ¬Šé‡å‘é‡çš„åå¾®åˆ†ã€‚æ¥ä¸‹ä¾†æˆ‘å€‘å¯ä»¥æ‹†é–‹ä¾†è§£é€™å…©å€‹åå¾®åˆ†çš„éƒ¨åˆ†ï¼Œæœ€å¾Œçµåˆèµ·ä¾†å¾—åˆ°æƒ³è¦æ±‚è§£çš„ã€Œæˆæœ¬å‡½æ•¸ J é—œæ–¼æ¬Šé‡å‘é‡çš„æ¢¯åº¦ã€ã€‚\n",
    "\n",
    "## å°æ•¸å‡½æ•¸åå¾®åˆ†\n",
    "\n",
    "é—œæ–¼å°æ•¸å‡½æ•¸åå¾®åˆ†ï¼Œå…ˆå‚™çŸ¥è­˜ï¼ˆä¸å¦å¤–è­‰æ˜çš„éƒ¨åˆ†ï¼‰ç‚ºï¼š\n",
    "\n",
    "\\begin{gather}\n",
    "\\frac{d}{dx}log(x) = \\frac{1}{x} \\\\\n",
    "\\frac{d}{dx}log(1-x) = \\frac{-1}{1-x}\n",
    "\\end{gather}\n",
    "\n",
    "å› æ­¤æˆ‘å€‘å¯ä»¥å…ˆå°‡æˆæœ¬å‡½æ•¸ J é—œæ–¼æ¬Šé‡å‘é‡çš„åå¾®åˆ†æ¨å°ç‚ºæˆæœ¬å‡½æ•¸ J é—œæ–¼å°æ•¸å‡½æ•¸çš„åå¾®åˆ†å½¢å¼ï¼š\n",
    "\n",
    "\\begin{gather}\n",
    "J(w) = \\frac{1}{m} \\left( -ylog(\\hat{p}) - (1-y)log(1-\\hat{p}) \\right)\\\\\n",
    "\\nabla_w J(w) = \\frac{1}{m} \\left( -y\\frac{1}{\\hat{p}} \\frac{\\partial \\hat{p}}{\\partial z} - (1-y)\\frac{-1}{1-\\hat{p}} \\frac{\\partial(1-\\hat{p})}{\\partial z} \\right)\n",
    "\\end{gather}\n",
    "\n",
    "å®Œæˆå°æ•¸å‡½æ•¸åå¾®åˆ†çš„éƒ¨åˆ†ï¼Œæ¥ä¸‹ä¾†æ˜¯ Sigmoid å‡½æ•¸åå¾®åˆ†çš„éƒ¨åˆ†ã€‚\n",
    "\n",
    "## Sigmoid å‡½æ•¸åå¾®åˆ†\n",
    "\n",
    "é—œæ–¼ Sigmoid å‡½æ•¸åå¾®åˆ†ï¼Œå…ˆå‚™çŸ¥è­˜ï¼ˆä¸å¦å¤–è­‰æ˜çš„éƒ¨åˆ†ï¼‰ç‚ºï¼š\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{dx}\\sigma(x) &= \\frac{d}{dx} \\left( (1+e^{-x})^{-1} \\right) \\\\\n",
    "&= \\frac{0 \\cdot (1+e^{-x})-1 \\cdot (e^{-x} \\cdot (-1) )}{(1 + e^{-x})^2}\\\\\n",
    "&= \\frac{e^{-x}}{(1+e^{-x})^2}\\\\\n",
    "&= \\frac{1-1+e^{-x}}{(1+e^{-x})^2}\\\\\n",
    "&= \\frac{1+e^{-x}}{(1+e^{-x})^2} - \\frac{1}{(1+e^{-x})^2} \\\\\n",
    "&= \\frac{1}{1+e^{-x}} - \\frac{1}{(1+e^{-x})^2} \\\\\n",
    "&= \\frac{1}{1+e^{-x}} \\left( 1 - \\frac{1}{1+e^{-x}} \\right) \\\\\n",
    "&= \\sigma(x)\\left( 1-\\sigma(x) \\right)\n",
    "\\end{align}\n",
    "\n",
    "å…ˆå‚™çŸ¥è­˜ä¸­çš„ç¬¬äºŒå¼é‹ç”¨åˆ°å¾®åˆ†çš„é™¤æ³•è¦å‰‡ã€ç¬¬å››å¼å‰‡é‹ç”¨åˆ°äº†ä¸€å€‹å·§å¦™çš„ +1-1 çš„æŠ€å·§ã€‚\n",
    "\n",
    "\\begin{gather}\n",
    "h(x) = \\frac{f(x)}{g(x)}\\\\\n",
    "h'(x) = \\frac{f(x)g'(x) - f'(x)g(x)}{g^2(x)}\n",
    "\\end{gather}\n",
    "\n",
    "å› æ­¤æˆ‘å€‘å¯ä»¥æ¥è‘—å°‡æˆæœ¬å‡½æ•¸ J é—œæ–¼æ¬Šé‡å‘é‡çš„åå¾®åˆ†æ¨å°ç‚ºæœ€çµ‚çš„å½¢å¼ï¼š\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_w J(w) &= \\frac{1}{m} \\left( -y\\frac{1}{\\hat{p}} \\frac{\\partial \\hat{p}}{\\partial z} - (1-y)\\frac{-1}{1-\\hat{p}} \\frac{\\partial(1-\\hat{p})}{\\partial z} \\right) \\\\\n",
    "&= \\frac{1}{m} \\left( -y\\frac{1}{\\hat{p}} \\sigma(z)(1-\\sigma(z))\\sigma'(z) + (1-y)\\frac{1}{1-\\hat{p}}\\sigma(z)(1-\\sigma(z))\\sigma'(z)\\right) \\\\\n",
    "&= \\frac{1}{m}\\sigma'(z) \\left( -y\\frac{1}{\\sigma(z)} \\sigma(z)(1-\\sigma(z)) + (1-y)\\frac{1}{1-\\sigma(z)}\\sigma(z)(1-\\sigma(z))\\right) \\\\\n",
    "&= \\frac{1}{m}\\sigma'(z) \\left( -y(1-\\sigma(z)) + (1-y)\\sigma(z) \\right) \\\\\n",
    "&=\\frac{1}{m}\\sigma'(z)\\left( -y+y\\sigma(z) + \\sigma(z)-y\\sigma(z) \\right) \\\\\n",
    "&=\\frac{1}{m}\\sigma'(z)\\left(\\sigma(z)-y \\right)\\\\\n",
    "&=\\frac{1}{m}X^{\\top}\\left(\\sigma(Xw)-y \\right)\n",
    "\\end{align}\n",
    "\n",
    "æˆ‘å€‘çµ‚æ–¼æ¨å°å‡ºä»¥äº¤å‰ç†µä½œç‚ºæˆæœ¬å‡½æ•¸ J é—œæ–¼æ¬Šé‡å‘é‡çš„æ¢¯åº¦ï¼Œæœ€å¾Œå°‡ç¾…å‰æ–¯è¿´æ­¸æ¢¯åº¦éæ¸›çš„æ³›å‹å¼è¡¨ç¤ºå¦‚ä¸‹ã€‚\n",
    "\n",
    "\\begin{gather}\n",
    "\\text{For each epoch:} \\\\\n",
    "w := w - \\eta \\nabla_w J(w) \\; \\text{, where } \\eta \\in \\mathbb{R}^+ \\\\\n",
    "w := w - \\eta \\frac{1}{m}X^{\\top} \\left( \\sigma(Xw) - y \\right)\n",
    "\\end{gather}\n",
    "\n",
    "## ä»¥ Numpy å¯¦ä½œç¾…å‰æ–¯è¿´æ­¸\n",
    "\n",
    "æˆåŠŸæ¨å°å‡ºä»¥äº¤å‰ç†µä½œç‚ºæˆæœ¬å‡½æ•¸ J é—œæ–¼æ¬Šé‡å‘é‡çš„æ¢¯åº¦ä¹‹å¾Œï¼Œæœ€å¾Œæˆ‘å€‘å¯«ä½œä¸€å€‹é¡åˆ¥ `LogisticRegression`ï¼Œä»¥ Python èˆ‡ Numpy æ¨¡çµ„å¯¦ä½œï¼Œåœ¨çµ¦å®šé©ç•¶è³‡æ–™é›†çš„ç‰¹å¾µçŸ©é™£èˆ‡ç›®æ¨™å‘é‡ï¼Œèƒ½å¤ è¿­ä»£å„ªåŒ–éš¨æ©Ÿç”Ÿæˆçš„æ¬Šé‡å‘é‡ï¼Œé€²è€Œç”Ÿæˆ h å‡½æ•¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd0a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    This class defines the vanilla descent algorithm for logistic regression.\n",
    "    Args:\n",
    "        fit_intercept (bool): Whether to add intercept for this model.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self._fit_intercept = fit_intercept\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"\n",
    "        This function returns the Sigmoid output as a probability given certain model weights.\n",
    "        \"\"\"\n",
    "        X_w = np.dot(X, self._w)\n",
    "        p_hat = 1 / (1 + np.exp(-X_w))\n",
    "        return p_hat\n",
    "    def find_gradient(self):\n",
    "        \"\"\"\n",
    "        This function returns the gradient given certain model weights.\n",
    "        \"\"\"\n",
    "        m = self._m\n",
    "        p_hat = self.sigmoid(self._X_train)\n",
    "        X_train_T = np.transpose(self._X_train)\n",
    "        gradient = (1/m) * np.dot(X_train_T, p_hat - self._y_train)\n",
    "        return gradient\n",
    "    def cross_entropy(self, epsilon=1e-06):\n",
    "        \"\"\"\n",
    "        This function returns the cross entropy given certain model weights.\n",
    "        \"\"\"\n",
    "        m = self._m\n",
    "        p_hat = self.sigmoid(self._X_train)\n",
    "        cost_y1 = -np.dot(self._y_train, np.log(p_hat + epsilon))\n",
    "        cost_y0 = -np.dot(1 - self._y_train, np.log(1 - p_hat + epsilon))\n",
    "        cross_entropy = (cost_y1 + cost_y0) / m\n",
    "        return cross_entropy\n",
    "    def fit(self, X_train, y_train, epochs=10000, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        This function uses vanilla gradient descent to solve for weights of this model.\n",
    "        Args:\n",
    "            X_train (ndarray): 2d-array for feature matrix of training data.\n",
    "            y_train (ndarray): 1d-array for target vector of training data.\n",
    "            epochs (int): The number of iterations to update the model weights.\n",
    "            learning_rate (float): The learning rate of gradient descent.\n",
    "        \"\"\"\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        m = self._X_train.shape[0]\n",
    "        self._m = m\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((self._m, 1), dtype=float)\n",
    "            self._X_train = np.concatenate([X0, self._X_train], axis=1)\n",
    "        n = self._X_train.shape[1]\n",
    "        self._w = np.random.rand(n)\n",
    "        n_prints = 10\n",
    "        print_iter = epochs // n_prints\n",
    "        for i in range(epochs):\n",
    "            cross_entropy = self.cross_entropy()\n",
    "            gradient = self.find_gradient()\n",
    "            if i % print_iter == 0:\n",
    "                print(\"epoch: {:6} - loss: {:.6f}\".format(i, cross_entropy))\n",
    "            self._w -= learning_rate*gradient\n",
    "        w_ravel = self._w.ravel().copy()\n",
    "        self.intercept_ = w_ravel[0]\n",
    "        self.coef_ = w_ravel[1:].reshape(1, -1)\n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted probability with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        m = X_test.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_test = np.concatenate([X0, X_test], axis=1)\n",
    "        p_hat_1 = self.sigmoid(self._X_test).reshape(-1, 1)\n",
    "        p_hat_0 = 1 - p_hat_1\n",
    "        proba = np.concatenate([p_hat_0, p_hat_1], axis=1)\n",
    "        return proba\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted label with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X_test)\n",
    "        y_pred = np.argmax(proba, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2af0f",
   "metadata": {},
   "source": [
    "`LogisticRegression` é¡åˆ¥çš„ä½¿ç”¨æ˜¯ä»¥ `fit()` æ–¹æ³•ç”Ÿæˆæ¬Šé‡å‘é‡ï¼Œä¸¦ä»¥ `predict()` æ–¹æ³•ç”Ÿæˆé æ¸¬ã€‚æˆ‘å€‘å¯ä»¥å°‡ `LogisticRegression` é¡åˆ¥æ‡‰ç”¨åœ¨ Kaggle çš„ Titanic è³‡æ–™é›†ï¼Œè—‰æ­¤ä¾†é©—è­‰å®ƒçš„å¯ç”¨æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4df32b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:      0 - loss: 1.664711\n",
      "epoch:   1000 - loss: 0.814446\n",
      "epoch:   2000 - loss: 0.761798\n",
      "epoch:   3000 - loss: 0.724259\n",
      "epoch:   4000 - loss: 0.697649\n",
      "epoch:   5000 - loss: 0.678728\n",
      "epoch:   6000 - loss: 0.665173\n",
      "epoch:   7000 - loss: 0.655375\n",
      "epoch:   8000 - loss: 0.648231\n",
      "epoch:   9000 - loss: 0.642980\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/datainpoint/week-forty-four-logistic-regression-part-two/main/train.csv\")\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/datainpoint/week-forty-four-logistic-regression-part-two/main/test.csv\")\n",
    "X_train = train[\"Fare\"].values.reshape(-1, 1)\n",
    "y_train = train[\"Survived\"].values\n",
    "X_test = test[\"Fare\"].values.reshape(-1, 1)\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "y_pred = logistic_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b68ba55",
   "metadata": {},
   "source": [
    "ç¬¬å››åå››é€±ç´„ç¶­å®‰è¨ˆç•«ï¼šç¾…å‰æ–¯è¿´æ­¸ä¾†åˆ°å°¾è²ï¼Œå¸Œæœ›æ‚¨ä¹Ÿå’Œæˆ‘ä¸€æ¨£æœŸå¾…ä¸‹ä¸€ç¯‡æ–‡ç« ã€‚å°æ–¼é€™ç¯‡æ–‡ç« æœ‰ä»€éº¼æƒ³æ³•å‘¢ï¼Ÿå–œæ­¡ğŸ˜»ã€åˆ†äº«ğŸ™Œã€è¨‚é–±ğŸ“¨æˆ–è€…ç•™è¨€ğŸ™‹â€â™‚ï¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
